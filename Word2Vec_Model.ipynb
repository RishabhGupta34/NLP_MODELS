{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RishabhGupta34/NLP_MODELS/blob/master/Word2Vec_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wFVcP5MCHkKq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qv5P7XEl8AM5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFTPwxhiHTek",
        "colab_type": "code",
        "outputId": "d6123ffd-7341-4cb3-f874-e6facfedfbc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir .kaggle\n",
        "import json\n",
        "token = {\"username\":\"your_username\",\"key\":\"your_key\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "!kaggle competitions download -c word2vec-nlp-tutorial"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n",
            "mkdir: cannot create directory ‘.kaggle’: File exists\n",
            "- path is now set to: {/content}\n",
            "Downloading sampleSubmission.csv to {/content}/competitions/word2vec-nlp-tutorial\n",
            "  0% 0.00/276k [00:00<?, ?B/s]\n",
            "100% 276k/276k [00:00<00:00, 87.0MB/s]\n",
            "Downloading unlabeledTrainData.tsv.zip to {/content}/competitions/word2vec-nlp-tutorial\n",
            " 69% 18.0M/26.0M [00:00<00:00, 58.8MB/s]\n",
            "100% 26.0M/26.0M [00:00<00:00, 98.5MB/s]\n",
            "Downloading testData.tsv.zip to {/content}/competitions/word2vec-nlp-tutorial\n",
            " 40% 5.00M/12.6M [00:00<00:00, 38.4MB/s]\n",
            "100% 12.6M/12.6M [00:00<00:00, 61.7MB/s]\n",
            "Downloading labeledTrainData.tsv.zip to {/content}/competitions/word2vec-nlp-tutorial\n",
            " 39% 5.00M/13.0M [00:00<00:00, 30.6MB/s]\n",
            "100% 13.0M/13.0M [00:00<00:00, 63.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xia5tgHMHcTs",
        "colab_type": "code",
        "outputId": "3f6ac645-1408-41aa-83d2-6ea250c7d1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "!ls {/content}/competitions/word2vec-nlp-tutorial"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledTrainData.tsv.zip  testData.tsv.zip\n",
            "sampleSubmission.csv\t  unlabeledTrainData.tsv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6X8nLKxYIkmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "8c1b3cfe-2b9f-4574-ed7f-21bdd0f907b6"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.ensemble import  RandomForestClassifier\n",
        "from gensim.models import word2vec\n",
        "import logging\n",
        "from gensim.models import Word2Vec\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "VjKWlxWuHilR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zip_ref = zipfile.ZipFile('{/content}/competitions/word2vec-nlp-tutorial/labeledTrainData.tsv.zip', 'r')\n",
        "zip_ref.extractall('./')\n",
        "zip_ref.close()\n",
        "zip_ref = zipfile.ZipFile('{/content}/competitions/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip', 'r')\n",
        "zip_ref.extractall('./')\n",
        "zip_ref.close()\n",
        "zip_ref = zipfile.ZipFile('{/content}/competitions/word2vec-nlp-tutorial/testData.tsv.zip', 'r')\n",
        "zip_ref.extractall('./')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dOy4N7-D7yjI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('labeledTrainData.tsv',delimiter='\\t',quoting=3)\n",
        "test_data=pd.read_csv('testData.tsv',delimiter='\\t',quoting=3)\n",
        "un_train_data=pd.read_csv('unlabeledTrainData.tsv',delimiter='\\t',quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RDuqNTgPOY6O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wu-pSdlSQ_Sn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(data,remove_stopwords=False):\n",
        "  all_stop=stopwords.words('english')   #list of stopwords\n",
        "  ss=SnowballStemmer(\"english\")   #list of stemmer\n",
        "  soup=BeautifulSoup(data)\n",
        "  html_text=soup.get_text().lower()    #for removing html tags \n",
        "  slash=re.sub('[\\\\\\\\]',\"\",html_text)  # for removing backward slash\n",
        "  quotes_text=re.sub('[\\\"¨]',\" <QUOT> \",slash) # for removing double quotes\n",
        "  com=re.sub('[,]',\" <COM> \",quotes_text) # for removing comma\n",
        "  apos=re.sub(\"[']\",\" <APOS> \",com) # for removing apostrophe\n",
        "  dott=re.sub('\\.+',\" <DOT> \",apos)  # for removing full stop\n",
        "  exc=re.sub('!+',\" <EXC> \",dott)   # for removing exclamation\n",
        "  num=re.sub(\"\\d+\",\" <NUM> \",exc)  # for removing number\n",
        "  brac=re.sub('[()]+',\" <BRAC> \",num)  # for removing brackets\n",
        "  ques=re.sub(\"\\?+\",\" <QUES> \",brac)  # for removing question mark\n",
        "  split_text=ques.split()  # for splitting the text\n",
        "  if remove_stopwords:\n",
        "    split_text=[w for w in split_text if w not in all_stop]  # for removing stopwords\n",
        "  stem_text=[ss.stem(i) for i in split_text]  # for stemming each word\n",
        "  return stem_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "osh_q3MqPZwa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def break_sent(train_data):\n",
        "  tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "  cleaned_data=[]\n",
        "  for i in range(train_data.shape[0]):\n",
        "    raw_sentences=tokenizer.tokenize(train_data[i])\n",
        "    for j in raw_sentences:\n",
        "      if(len(j)>0):\n",
        "        cleaned_data.append(clean_text(j))\n",
        "  return cleaned_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7c0hrQARwtQ",
        "colab_type": "code",
        "outputId": "726c280e-45cd-447e-9785-698325169a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "all_cleaned_sentences=[]\n",
        "all_cleaned_sentences+=break_sent(train_data.review)\n",
        "all_cleaned_sentences+=break_sent(un_train_data.review)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "I7DaZsLxSAJM",
        "colab_type": "code",
        "outputId": "238e529e-df80-4a79-dce1-455ca840aa4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(all_cleaned_sentences))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "795538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vM_8UPdgWp9c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_features = 300    # Word vector dimensionality                      \n",
        "min_word_count = 40   # Minimum word count                        \n",
        "num_workers = 6       # Number of threads to run in parallel\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QqHfpPmGgH4T",
        "colab_type": "code",
        "outputId": "e6ebc70a-a4e0-4c51-ee54-f06e9ce27123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5331
        }
      },
      "cell_type": "code",
      "source": [
        "model = word2vec.Word2Vec(all_cleaned_sentences, workers=num_workers,size=num_features,min_count = min_word_count,window = context, sample = downsampling)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-27 22:09:28,673 : INFO : collecting all words and their counts\n",
            "2019-04-27 22:09:28,675 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-04-27 22:09:28,754 : INFO : PROGRESS: at sentence #10000, processed 261919 words, keeping 14282 word types\n",
            "2019-04-27 22:09:28,820 : INFO : PROGRESS: at sentence #20000, processed 523814 words, keeping 20411 word types\n",
            "2019-04-27 22:09:28,887 : INFO : PROGRESS: at sentence #30000, processed 778285 words, keeping 25103 word types\n",
            "2019-04-27 22:09:28,957 : INFO : PROGRESS: at sentence #40000, processed 1040208 words, keeping 29288 word types\n",
            "2019-04-27 22:09:29,026 : INFO : PROGRESS: at sentence #50000, processed 1294552 words, keeping 32926 word types\n",
            "2019-04-27 22:09:29,098 : INFO : PROGRESS: at sentence #60000, processed 1550740 words, keeping 36061 word types\n",
            "2019-04-27 22:09:29,171 : INFO : PROGRESS: at sentence #70000, processed 1809233 words, keeping 39001 word types\n",
            "2019-04-27 22:09:29,240 : INFO : PROGRESS: at sentence #80000, processed 2063285 words, keeping 41701 word types\n",
            "2019-04-27 22:09:29,315 : INFO : PROGRESS: at sentence #90000, processed 2323620 words, keeping 44567 word types\n",
            "2019-04-27 22:09:29,389 : INFO : PROGRESS: at sentence #100000, processed 2580369 words, keeping 47134 word types\n",
            "2019-04-27 22:09:29,461 : INFO : PROGRESS: at sentence #110000, processed 2835142 words, keeping 49546 word types\n",
            "2019-04-27 22:09:29,536 : INFO : PROGRESS: at sentence #120000, processed 3092224 words, keeping 51998 word types\n",
            "2019-04-27 22:09:29,612 : INFO : PROGRESS: at sentence #130000, processed 3354488 words, keeping 54331 word types\n",
            "2019-04-27 22:09:29,683 : INFO : PROGRESS: at sentence #140000, processed 3601131 words, keeping 56371 word types\n",
            "2019-04-27 22:09:29,760 : INFO : PROGRESS: at sentence #150000, processed 3862043 words, keeping 58619 word types\n",
            "2019-04-27 22:09:29,837 : INFO : PROGRESS: at sentence #160000, processed 4120426 words, keeping 60680 word types\n",
            "2019-04-27 22:09:29,913 : INFO : PROGRESS: at sentence #170000, processed 4379294 words, keeping 62649 word types\n",
            "2019-04-27 22:09:29,987 : INFO : PROGRESS: at sentence #180000, processed 4634910 words, keeping 64563 word types\n",
            "2019-04-27 22:09:30,061 : INFO : PROGRESS: at sentence #190000, processed 4895866 words, keeping 66369 word types\n",
            "2019-04-27 22:09:30,134 : INFO : PROGRESS: at sentence #200000, processed 5155690 words, keeping 68172 word types\n",
            "2019-04-27 22:09:30,210 : INFO : PROGRESS: at sentence #210000, processed 5412048 words, keeping 70043 word types\n",
            "2019-04-27 22:09:30,283 : INFO : PROGRESS: at sentence #220000, processed 5672483 words, keeping 71968 word types\n",
            "2019-04-27 22:09:30,358 : INFO : PROGRESS: at sentence #230000, processed 5930074 words, keeping 73772 word types\n",
            "2019-04-27 22:09:30,436 : INFO : PROGRESS: at sentence #240000, processed 6193839 words, keeping 75549 word types\n",
            "2019-04-27 22:09:30,507 : INFO : PROGRESS: at sentence #250000, processed 6442336 words, keeping 77276 word types\n",
            "2019-04-27 22:09:30,581 : INFO : PROGRESS: at sentence #260000, processed 6697607 words, keeping 79087 word types\n",
            "2019-04-27 22:09:30,657 : INFO : PROGRESS: at sentence #270000, processed 6954269 words, keeping 80913 word types\n",
            "2019-04-27 22:09:30,731 : INFO : PROGRESS: at sentence #280000, processed 7215225 words, keeping 83055 word types\n",
            "2019-04-27 22:09:30,805 : INFO : PROGRESS: at sentence #290000, processed 7473203 words, keeping 85088 word types\n",
            "2019-04-27 22:09:30,882 : INFO : PROGRESS: at sentence #300000, processed 7733910 words, keeping 86931 word types\n",
            "2019-04-27 22:09:30,963 : INFO : PROGRESS: at sentence #310000, processed 7995283 words, keeping 88823 word types\n",
            "2019-04-27 22:09:31,039 : INFO : PROGRESS: at sentence #320000, processed 8255238 words, keeping 90764 word types\n",
            "2019-04-27 22:09:31,113 : INFO : PROGRESS: at sentence #330000, processed 8511685 words, keeping 92553 word types\n",
            "2019-04-27 22:09:31,191 : INFO : PROGRESS: at sentence #340000, processed 8777615 words, keeping 94340 word types\n",
            "2019-04-27 22:09:31,266 : INFO : PROGRESS: at sentence #350000, processed 9035806 words, keeping 96042 word types\n",
            "2019-04-27 22:09:31,341 : INFO : PROGRESS: at sentence #360000, processed 9291829 words, keeping 97705 word types\n",
            "2019-04-27 22:09:31,419 : INFO : PROGRESS: at sentence #370000, processed 9554536 words, keeping 99453 word types\n",
            "2019-04-27 22:09:31,497 : INFO : PROGRESS: at sentence #380000, processed 9815360 words, keeping 101220 word types\n",
            "2019-04-27 22:09:31,574 : INFO : PROGRESS: at sentence #390000, processed 10081479 words, keeping 102895 word types\n",
            "2019-04-27 22:09:31,650 : INFO : PROGRESS: at sentence #400000, processed 10339270 words, keeping 104550 word types\n",
            "2019-04-27 22:09:31,723 : INFO : PROGRESS: at sentence #410000, processed 10595665 words, keeping 106064 word types\n",
            "2019-04-27 22:09:31,796 : INFO : PROGRESS: at sentence #420000, processed 10851931 words, keeping 107735 word types\n",
            "2019-04-27 22:09:31,878 : INFO : PROGRESS: at sentence #430000, processed 11115856 words, keeping 109353 word types\n",
            "2019-04-27 22:09:31,956 : INFO : PROGRESS: at sentence #440000, processed 11378978 words, keeping 111009 word types\n",
            "2019-04-27 22:09:32,032 : INFO : PROGRESS: at sentence #450000, processed 11637441 words, keeping 112635 word types\n",
            "2019-04-27 22:09:32,112 : INFO : PROGRESS: at sentence #460000, processed 11907032 words, keeping 114311 word types\n",
            "2019-04-27 22:09:32,189 : INFO : PROGRESS: at sentence #470000, processed 12171336 words, keeping 115766 word types\n",
            "2019-04-27 22:09:32,266 : INFO : PROGRESS: at sentence #480000, processed 12426831 words, keeping 117232 word types\n",
            "2019-04-27 22:09:32,343 : INFO : PROGRESS: at sentence #490000, processed 12690425 words, keeping 118860 word types\n",
            "2019-04-27 22:09:32,417 : INFO : PROGRESS: at sentence #500000, processed 12947660 words, keeping 120276 word types\n",
            "2019-04-27 22:09:32,491 : INFO : PROGRESS: at sentence #510000, processed 13208465 words, keeping 121849 word types\n",
            "2019-04-27 22:09:32,566 : INFO : PROGRESS: at sentence #520000, processed 13467488 words, keeping 123276 word types\n",
            "2019-04-27 22:09:32,640 : INFO : PROGRESS: at sentence #530000, processed 13727840 words, keeping 124610 word types\n",
            "2019-04-27 22:09:32,716 : INFO : PROGRESS: at sentence #540000, processed 13988069 words, keeping 126216 word types\n",
            "2019-04-27 22:09:32,791 : INFO : PROGRESS: at sentence #550000, processed 14249429 words, keeping 127651 word types\n",
            "2019-04-27 22:09:32,864 : INFO : PROGRESS: at sentence #560000, processed 14505832 words, keeping 129104 word types\n",
            "2019-04-27 22:09:32,939 : INFO : PROGRESS: at sentence #570000, processed 14770434 words, keeping 130593 word types\n",
            "2019-04-27 22:09:33,014 : INFO : PROGRESS: at sentence #580000, processed 15026703 words, keeping 132064 word types\n",
            "2019-04-27 22:09:33,093 : INFO : PROGRESS: at sentence #590000, processed 15287747 words, keeping 133485 word types\n",
            "2019-04-27 22:09:33,169 : INFO : PROGRESS: at sentence #600000, processed 15545232 words, keeping 134823 word types\n",
            "2019-04-27 22:09:33,245 : INFO : PROGRESS: at sentence #610000, processed 15801883 words, keeping 136298 word types\n",
            "2019-04-27 22:09:33,322 : INFO : PROGRESS: at sentence #620000, processed 16064247 words, keeping 137586 word types\n",
            "2019-04-27 22:09:33,398 : INFO : PROGRESS: at sentence #630000, processed 16323790 words, keeping 138866 word types\n",
            "2019-04-27 22:09:33,475 : INFO : PROGRESS: at sentence #640000, processed 16579565 words, keeping 140185 word types\n",
            "2019-04-27 22:09:33,553 : INFO : PROGRESS: at sentence #650000, processed 16841323 words, keeping 141561 word types\n",
            "2019-04-27 22:09:33,631 : INFO : PROGRESS: at sentence #660000, processed 17099473 words, keeping 142914 word types\n",
            "2019-04-27 22:09:33,705 : INFO : PROGRESS: at sentence #670000, processed 17358148 words, keeping 144108 word types\n",
            "2019-04-27 22:09:33,782 : INFO : PROGRESS: at sentence #680000, processed 17619215 words, keeping 145424 word types\n",
            "2019-04-27 22:09:33,857 : INFO : PROGRESS: at sentence #690000, processed 17876740 words, keeping 146741 word types\n",
            "2019-04-27 22:09:33,935 : INFO : PROGRESS: at sentence #700000, processed 18140909 words, keeping 148130 word types\n",
            "2019-04-27 22:09:34,012 : INFO : PROGRESS: at sentence #710000, processed 18399694 words, keeping 149302 word types\n",
            "2019-04-27 22:09:34,087 : INFO : PROGRESS: at sentence #720000, processed 18660813 words, keeping 150487 word types\n",
            "2019-04-27 22:09:34,164 : INFO : PROGRESS: at sentence #730000, processed 18922562 words, keeping 151752 word types\n",
            "2019-04-27 22:09:34,243 : INFO : PROGRESS: at sentence #740000, processed 19178358 words, keeping 152995 word types\n",
            "2019-04-27 22:09:34,321 : INFO : PROGRESS: at sentence #750000, processed 19431579 words, keeping 154142 word types\n",
            "2019-04-27 22:09:34,398 : INFO : PROGRESS: at sentence #760000, processed 19685933 words, keeping 155284 word types\n",
            "2019-04-27 22:09:34,476 : INFO : PROGRESS: at sentence #770000, processed 19949210 words, keeping 156645 word types\n",
            "2019-04-27 22:09:34,553 : INFO : PROGRESS: at sentence #780000, processed 20215450 words, keeping 157851 word types\n",
            "2019-04-27 22:09:34,631 : INFO : PROGRESS: at sentence #790000, processed 20478362 words, keeping 159111 word types\n",
            "2019-04-27 22:09:34,675 : INFO : collected 159812 word types from a corpus of 20621037 raw words and 795538 sentences\n",
            "2019-04-27 22:09:34,676 : INFO : Loading a fresh vocabulary\n",
            "2019-04-27 22:09:34,797 : INFO : effective_min_count=40 retains 12286 unique words (7% of original 159812, drops 147526)\n",
            "2019-04-27 22:09:34,799 : INFO : effective_min_count=40 leaves 20118577 word corpus (97% of original 20621037, drops 502460)\n",
            "2019-04-27 22:09:34,847 : INFO : deleting the raw counts dictionary of 159812 items\n",
            "2019-04-27 22:09:34,855 : INFO : sample=0.001 downsamples 49 most-common words\n",
            "2019-04-27 22:09:34,857 : INFO : downsampling leaves estimated 13747637 word corpus (68.3% of prior 20118577)\n",
            "2019-04-27 22:09:34,913 : INFO : estimated required memory for 12286 words and 300 dimensions: 35629400 bytes\n",
            "2019-04-27 22:09:34,914 : INFO : resetting layer weights\n",
            "2019-04-27 22:09:35,071 : INFO : training model with 6 workers on 12286 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2019-04-27 22:09:36,125 : INFO : EPOCH 1 - PROGRESS: at 2.31% examples, 307331 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:37,127 : INFO : EPOCH 1 - PROGRESS: at 4.83% examples, 326018 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:09:38,130 : INFO : EPOCH 1 - PROGRESS: at 7.17% examples, 323269 words/s, in_qsize 9, out_qsize 2\n",
            "2019-04-27 22:09:39,155 : INFO : EPOCH 1 - PROGRESS: at 9.66% examples, 325376 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:09:40,169 : INFO : EPOCH 1 - PROGRESS: at 12.21% examples, 328504 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:41,173 : INFO : EPOCH 1 - PROGRESS: at 14.65% examples, 329002 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:42,189 : INFO : EPOCH 1 - PROGRESS: at 17.20% examples, 330612 words/s, in_qsize 12, out_qsize 1\n",
            "2019-04-27 22:09:43,237 : INFO : EPOCH 1 - PROGRESS: at 19.82% examples, 332178 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:44,294 : INFO : EPOCH 1 - PROGRESS: at 22.41% examples, 332369 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:45,306 : INFO : EPOCH 1 - PROGRESS: at 24.82% examples, 332017 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:46,309 : INFO : EPOCH 1 - PROGRESS: at 27.30% examples, 332521 words/s, in_qsize 11, out_qsize 2\n",
            "2019-04-27 22:09:47,352 : INFO : EPOCH 1 - PROGRESS: at 29.86% examples, 333009 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:48,366 : INFO : EPOCH 1 - PROGRESS: at 32.57% examples, 335027 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:09:49,381 : INFO : EPOCH 1 - PROGRESS: at 34.90% examples, 333534 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:50,415 : INFO : EPOCH 1 - PROGRESS: at 37.46% examples, 334048 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:51,425 : INFO : EPOCH 1 - PROGRESS: at 39.92% examples, 334153 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:52,430 : INFO : EPOCH 1 - PROGRESS: at 42.28% examples, 333616 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:53,458 : INFO : EPOCH 1 - PROGRESS: at 44.90% examples, 334500 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:54,503 : INFO : EPOCH 1 - PROGRESS: at 47.39% examples, 334284 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:55,550 : INFO : EPOCH 1 - PROGRESS: at 49.93% examples, 334425 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:56,557 : INFO : EPOCH 1 - PROGRESS: at 52.47% examples, 334852 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:57,558 : INFO : EPOCH 1 - PROGRESS: at 54.96% examples, 335303 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:58,624 : INFO : EPOCH 1 - PROGRESS: at 57.44% examples, 334824 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:09:59,628 : INFO : EPOCH 1 - PROGRESS: at 59.86% examples, 334927 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:00,636 : INFO : EPOCH 1 - PROGRESS: at 62.27% examples, 334719 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:01,672 : INFO : EPOCH 1 - PROGRESS: at 64.80% examples, 334702 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:02,697 : INFO : EPOCH 1 - PROGRESS: at 67.20% examples, 334318 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:03,741 : INFO : EPOCH 1 - PROGRESS: at 69.77% examples, 334440 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:04,773 : INFO : EPOCH 1 - PROGRESS: at 72.26% examples, 334463 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:05,818 : INFO : EPOCH 1 - PROGRESS: at 74.85% examples, 334567 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:06,840 : INFO : EPOCH 1 - PROGRESS: at 77.32% examples, 334475 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:07,851 : INFO : EPOCH 1 - PROGRESS: at 79.84% examples, 334698 words/s, in_qsize 9, out_qsize 2\n",
            "2019-04-27 22:10:08,855 : INFO : EPOCH 1 - PROGRESS: at 82.31% examples, 334802 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:09,855 : INFO : EPOCH 1 - PROGRESS: at 84.67% examples, 334559 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:10:10,884 : INFO : EPOCH 1 - PROGRESS: at 87.17% examples, 334605 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:11,898 : INFO : EPOCH 1 - PROGRESS: at 89.62% examples, 334599 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:12,921 : INFO : EPOCH 1 - PROGRESS: at 92.18% examples, 334890 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:13,946 : INFO : EPOCH 1 - PROGRESS: at 94.79% examples, 335134 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:14,948 : INFO : EPOCH 1 - PROGRESS: at 97.25% examples, 335225 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:15,979 : INFO : EPOCH 1 - PROGRESS: at 99.67% examples, 335078 words/s, in_qsize 6, out_qsize 1\n",
            "2019-04-27 22:10:15,987 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-04-27 22:10:16,007 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-04-27 22:10:16,012 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-04-27 22:10:16,039 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-04-27 22:10:16,050 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-04-27 22:10:16,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-27 22:10:16,054 : INFO : EPOCH - 1 : training on 20621037 raw words (13746540 effective words) took 41.0s, 335527 effective words/s\n",
            "2019-04-27 22:10:17,084 : INFO : EPOCH 2 - PROGRESS: at 2.27% examples, 307337 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:18,095 : INFO : EPOCH 2 - PROGRESS: at 4.73% examples, 321431 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:19,104 : INFO : EPOCH 2 - PROGRESS: at 7.23% examples, 326292 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:10:20,120 : INFO : EPOCH 2 - PROGRESS: at 9.72% examples, 328237 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:10:21,146 : INFO : EPOCH 2 - PROGRESS: at 12.21% examples, 328727 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:22,197 : INFO : EPOCH 2 - PROGRESS: at 14.74% examples, 328883 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:23,236 : INFO : EPOCH 2 - PROGRESS: at 17.40% examples, 331347 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:24,283 : INFO : EPOCH 2 - PROGRESS: at 19.92% examples, 331230 words/s, in_qsize 11, out_qsize 2\n",
            "2019-04-27 22:10:25,314 : INFO : EPOCH 2 - PROGRESS: at 22.46% examples, 331696 words/s, in_qsize 9, out_qsize 2\n",
            "2019-04-27 22:10:26,319 : INFO : EPOCH 2 - PROGRESS: at 24.96% examples, 332900 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:27,341 : INFO : EPOCH 2 - PROGRESS: at 27.44% examples, 332796 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:28,396 : INFO : EPOCH 2 - PROGRESS: at 30.00% examples, 332944 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:29,398 : INFO : EPOCH 2 - PROGRESS: at 32.47% examples, 332779 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:30,420 : INFO : EPOCH 2 - PROGRESS: at 34.96% examples, 332735 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:31,442 : INFO : EPOCH 2 - PROGRESS: at 37.32% examples, 331831 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:32,501 : INFO : EPOCH 2 - PROGRESS: at 39.87% examples, 331898 words/s, in_qsize 11, out_qsize 1\n",
            "2019-04-27 22:10:33,516 : INFO : EPOCH 2 - PROGRESS: at 42.32% examples, 332043 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:34,533 : INFO : EPOCH 2 - PROGRESS: at 44.85% examples, 332504 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:35,558 : INFO : EPOCH 2 - PROGRESS: at 47.25% examples, 332090 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:36,595 : INFO : EPOCH 2 - PROGRESS: at 49.69% examples, 331826 words/s, in_qsize 9, out_qsize 2\n",
            "2019-04-27 22:10:37,610 : INFO : EPOCH 2 - PROGRESS: at 52.23% examples, 332265 words/s, in_qsize 12, out_qsize 1\n",
            "2019-04-27 22:10:38,620 : INFO : EPOCH 2 - PROGRESS: at 54.71% examples, 332691 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:39,686 : INFO : EPOCH 2 - PROGRESS: at 57.26% examples, 332605 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:40,711 : INFO : EPOCH 2 - PROGRESS: at 59.81% examples, 333332 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:41,763 : INFO : EPOCH 2 - PROGRESS: at 62.17% examples, 332358 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:42,766 : INFO : EPOCH 2 - PROGRESS: at 64.61% examples, 332325 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:43,788 : INFO : EPOCH 2 - PROGRESS: at 66.96% examples, 331829 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:44,853 : INFO : EPOCH 2 - PROGRESS: at 69.58% examples, 332027 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:45,861 : INFO : EPOCH 2 - PROGRESS: at 72.12% examples, 332616 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:46,861 : INFO : EPOCH 2 - PROGRESS: at 74.55% examples, 332629 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:47,873 : INFO : EPOCH 2 - PROGRESS: at 76.97% examples, 332513 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:48,884 : INFO : EPOCH 2 - PROGRESS: at 79.45% examples, 332623 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:49,895 : INFO : EPOCH 2 - PROGRESS: at 81.86% examples, 332512 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:50,902 : INFO : EPOCH 2 - PROGRESS: at 84.28% examples, 332461 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:51,941 : INFO : EPOCH 2 - PROGRESS: at 86.81% examples, 332463 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:10:52,972 : INFO : EPOCH 2 - PROGRESS: at 89.25% examples, 332365 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:10:53,997 : INFO : EPOCH 2 - PROGRESS: at 91.75% examples, 332512 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:55,037 : INFO : EPOCH 2 - PROGRESS: at 94.30% examples, 332528 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:56,042 : INFO : EPOCH 2 - PROGRESS: at 96.83% examples, 332807 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:10:57,113 : INFO : EPOCH 2 - PROGRESS: at 99.34% examples, 332731 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:57,281 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-04-27 22:10:57,286 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-04-27 22:10:57,290 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-04-27 22:10:57,308 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-04-27 22:10:57,311 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-04-27 22:10:57,331 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-27 22:10:57,332 : INFO : EPOCH - 2 : training on 20621037 raw words (13747327 effective words) took 41.3s, 333136 effective words/s\n",
            "2019-04-27 22:10:58,379 : INFO : EPOCH 3 - PROGRESS: at 2.36% examples, 316759 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:10:59,398 : INFO : EPOCH 3 - PROGRESS: at 4.97% examples, 334459 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:00,414 : INFO : EPOCH 3 - PROGRESS: at 7.42% examples, 332210 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:01,435 : INFO : EPOCH 3 - PROGRESS: at 9.87% examples, 330535 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:11:02,478 : INFO : EPOCH 3 - PROGRESS: at 12.44% examples, 332054 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:03,485 : INFO : EPOCH 3 - PROGRESS: at 14.94% examples, 332900 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:04,519 : INFO : EPOCH 3 - PROGRESS: at 17.45% examples, 332164 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:05,542 : INFO : EPOCH 3 - PROGRESS: at 19.92% examples, 332105 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:06,575 : INFO : EPOCH 3 - PROGRESS: at 22.45% examples, 332399 words/s, in_qsize 12, out_qsize 1\n",
            "2019-04-27 22:11:07,584 : INFO : EPOCH 3 - PROGRESS: at 24.97% examples, 333443 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:08,631 : INFO : EPOCH 3 - PROGRESS: at 27.48% examples, 333135 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:11:09,631 : INFO : EPOCH 3 - PROGRESS: at 29.90% examples, 333101 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:10,705 : INFO : EPOCH 3 - PROGRESS: at 32.62% examples, 333571 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:11,755 : INFO : EPOCH 3 - PROGRESS: at 35.14% examples, 333518 words/s, in_qsize 11, out_qsize 3\n",
            "2019-04-27 22:11:12,785 : INFO : EPOCH 3 - PROGRESS: at 37.71% examples, 333915 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:13,803 : INFO : EPOCH 3 - PROGRESS: at 40.11% examples, 333460 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:11:14,829 : INFO : EPOCH 3 - PROGRESS: at 42.60% examples, 333687 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:11:15,839 : INFO : EPOCH 3 - PROGRESS: at 45.14% examples, 334170 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:11:16,872 : INFO : EPOCH 3 - PROGRESS: at 47.59% examples, 333852 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:11:17,909 : INFO : EPOCH 3 - PROGRESS: at 50.12% examples, 334169 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:18,926 : INFO : EPOCH 3 - PROGRESS: at 52.63% examples, 334129 words/s, in_qsize 9, out_qsize 2\n",
            "2019-04-27 22:11:19,949 : INFO : EPOCH 3 - PROGRESS: at 55.11% examples, 334281 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:11:21,035 : INFO : EPOCH 3 - PROGRESS: at 57.62% examples, 334016 words/s, in_qsize 11, out_qsize 4\n",
            "2019-04-27 22:11:22,027 : INFO : EPOCH 3 - PROGRESS: at 60.06% examples, 334185 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:23,044 : INFO : EPOCH 3 - PROGRESS: at 62.52% examples, 334159 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:24,083 : INFO : EPOCH 3 - PROGRESS: at 65.03% examples, 334319 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:25,123 : INFO : EPOCH 3 - PROGRESS: at 67.55% examples, 334025 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:26,140 : INFO : EPOCH 3 - PROGRESS: at 70.06% examples, 334237 words/s, in_qsize 12, out_qsize 1\n",
            "2019-04-27 22:11:27,151 : INFO : EPOCH 3 - PROGRESS: at 72.51% examples, 334285 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:28,176 : INFO : EPOCH 3 - PROGRESS: at 74.94% examples, 333973 words/s, in_qsize 12, out_qsize 2\n",
            "2019-04-27 22:11:29,188 : INFO : EPOCH 3 - PROGRESS: at 77.46% examples, 334225 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:30,217 : INFO : EPOCH 3 - PROGRESS: at 80.03% examples, 334474 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:31,276 : INFO : EPOCH 3 - PROGRESS: at 82.60% examples, 334414 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:32,300 : INFO : EPOCH 3 - PROGRESS: at 85.19% examples, 334892 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:11:33,335 : INFO : EPOCH 3 - PROGRESS: at 87.57% examples, 334331 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:34,341 : INFO : EPOCH 3 - PROGRESS: at 90.00% examples, 334390 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:35,360 : INFO : EPOCH 3 - PROGRESS: at 92.39% examples, 334026 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:11:36,378 : INFO : EPOCH 3 - PROGRESS: at 94.88% examples, 334006 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:37,406 : INFO : EPOCH 3 - PROGRESS: at 97.39% examples, 334076 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:11:38,375 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-04-27 22:11:38,405 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-04-27 22:11:38,420 : INFO : EPOCH 3 - PROGRESS: at 99.87% examples, 334245 words/s, in_qsize 3, out_qsize 1\n",
            "2019-04-27 22:11:38,426 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-04-27 22:11:38,433 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-04-27 22:11:38,443 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-04-27 22:11:38,454 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-27 22:11:38,456 : INFO : EPOCH - 3 : training on 20621037 raw words (13745380 effective words) took 41.1s, 334369 effective words/s\n",
            "2019-04-27 22:11:39,473 : INFO : EPOCH 4 - PROGRESS: at 2.26% examples, 312755 words/s, in_qsize 12, out_qsize 2\n",
            "2019-04-27 22:11:40,504 : INFO : EPOCH 4 - PROGRESS: at 4.83% examples, 327737 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:11:41,522 : INFO : EPOCH 4 - PROGRESS: at 7.32% examples, 329343 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:42,531 : INFO : EPOCH 4 - PROGRESS: at 9.77% examples, 329353 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:11:43,549 : INFO : EPOCH 4 - PROGRESS: at 12.21% examples, 328845 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:44,571 : INFO : EPOCH 4 - PROGRESS: at 14.51% examples, 324999 words/s, in_qsize 12, out_qsize 1\n",
            "2019-04-27 22:11:45,622 : INFO : EPOCH 4 - PROGRESS: at 16.91% examples, 322812 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:46,657 : INFO : EPOCH 4 - PROGRESS: at 19.24% examples, 320951 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:47,668 : INFO : EPOCH 4 - PROGRESS: at 21.63% examples, 321141 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:48,677 : INFO : EPOCH 4 - PROGRESS: at 23.94% examples, 320648 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:11:49,681 : INFO : EPOCH 4 - PROGRESS: at 26.28% examples, 320388 words/s, in_qsize 12, out_qsize 1\n",
            "2019-04-27 22:11:50,689 : INFO : EPOCH 4 - PROGRESS: at 28.60% examples, 320058 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:51,713 : INFO : EPOCH 4 - PROGRESS: at 30.95% examples, 319435 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:11:52,743 : INFO : EPOCH 4 - PROGRESS: at 33.37% examples, 319125 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:53,797 : INFO : EPOCH 4 - PROGRESS: at 35.98% examples, 320622 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:54,829 : INFO : EPOCH 4 - PROGRESS: at 38.42% examples, 321122 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:55,879 : INFO : EPOCH 4 - PROGRESS: at 40.89% examples, 321245 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:11:56,907 : INFO : EPOCH 4 - PROGRESS: at 43.43% examples, 322490 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:57,938 : INFO : EPOCH 4 - PROGRESS: at 45.95% examples, 323170 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:58,955 : INFO : EPOCH 4 - PROGRESS: at 48.38% examples, 323661 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:11:59,959 : INFO : EPOCH 4 - PROGRESS: at 50.86% examples, 324339 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:12:00,989 : INFO : EPOCH 4 - PROGRESS: at 53.34% examples, 324549 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:01,994 : INFO : EPOCH 4 - PROGRESS: at 55.74% examples, 324805 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:03,018 : INFO : EPOCH 4 - PROGRESS: at 58.19% examples, 325343 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:12:04,039 : INFO : EPOCH 4 - PROGRESS: at 60.75% examples, 326128 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:05,044 : INFO : EPOCH 4 - PROGRESS: at 63.11% examples, 326057 words/s, in_qsize 11, out_qsize 1\n",
            "2019-04-27 22:12:06,073 : INFO : EPOCH 4 - PROGRESS: at 65.62% examples, 326437 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:07,095 : INFO : EPOCH 4 - PROGRESS: at 67.98% examples, 326152 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:08,107 : INFO : EPOCH 4 - PROGRESS: at 70.41% examples, 326238 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:12:09,117 : INFO : EPOCH 4 - PROGRESS: at 72.86% examples, 326565 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:10,140 : INFO : EPOCH 4 - PROGRESS: at 75.33% examples, 326725 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:11,168 : INFO : EPOCH 4 - PROGRESS: at 77.80% examples, 326816 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:12,204 : INFO : EPOCH 4 - PROGRESS: at 80.33% examples, 327037 words/s, in_qsize 9, out_qsize 2\n",
            "2019-04-27 22:12:13,263 : INFO : EPOCH 4 - PROGRESS: at 82.88% examples, 327214 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:14,292 : INFO : EPOCH 4 - PROGRESS: at 85.39% examples, 327484 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:15,321 : INFO : EPOCH 4 - PROGRESS: at 87.84% examples, 327552 words/s, in_qsize 9, out_qsize 2\n",
            "2019-04-27 22:12:16,332 : INFO : EPOCH 4 - PROGRESS: at 90.30% examples, 327764 words/s, in_qsize 11, out_qsize 2\n",
            "2019-04-27 22:12:17,339 : INFO : EPOCH 4 - PROGRESS: at 92.69% examples, 327668 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:18,365 : INFO : EPOCH 4 - PROGRESS: at 95.23% examples, 328085 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:12:19,350 : INFO : EPOCH 4 - PROGRESS: at 97.67% examples, 328315 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:20,268 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-04-27 22:12:20,286 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-04-27 22:12:20,306 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-04-27 22:12:20,308 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-04-27 22:12:20,311 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-04-27 22:12:20,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-27 22:12:20,323 : INFO : EPOCH - 4 : training on 20621037 raw words (13744198 effective words) took 41.9s, 328411 effective words/s\n",
            "2019-04-27 22:12:21,347 : INFO : EPOCH 5 - PROGRESS: at 2.17% examples, 297138 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:22,364 : INFO : EPOCH 5 - PROGRESS: at 4.63% examples, 315324 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:23,377 : INFO : EPOCH 5 - PROGRESS: at 7.17% examples, 323924 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:24,422 : INFO : EPOCH 5 - PROGRESS: at 9.66% examples, 324079 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:25,466 : INFO : EPOCH 5 - PROGRESS: at 12.16% examples, 324317 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:26,475 : INFO : EPOCH 5 - PROGRESS: at 14.65% examples, 326349 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:27,495 : INFO : EPOCH 5 - PROGRESS: at 17.15% examples, 327220 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:28,514 : INFO : EPOCH 5 - PROGRESS: at 19.59% examples, 327037 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:29,535 : INFO : EPOCH 5 - PROGRESS: at 22.07% examples, 327713 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:12:30,552 : INFO : EPOCH 5 - PROGRESS: at 24.63% examples, 329566 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:31,576 : INFO : EPOCH 5 - PROGRESS: at 27.16% examples, 330304 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:32,598 : INFO : EPOCH 5 - PROGRESS: at 29.66% examples, 331033 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:33,599 : INFO : EPOCH 5 - PROGRESS: at 32.18% examples, 331593 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:34,605 : INFO : EPOCH 5 - PROGRESS: at 34.61% examples, 331505 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:35,628 : INFO : EPOCH 5 - PROGRESS: at 37.03% examples, 331109 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:36,643 : INFO : EPOCH 5 - PROGRESS: at 39.55% examples, 331663 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:37,643 : INFO : EPOCH 5 - PROGRESS: at 41.95% examples, 331751 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:38,662 : INFO : EPOCH 5 - PROGRESS: at 44.26% examples, 330760 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:39,683 : INFO : EPOCH 5 - PROGRESS: at 46.72% examples, 330981 words/s, in_qsize 11, out_qsize 2\n",
            "2019-04-27 22:12:40,681 : INFO : EPOCH 5 - PROGRESS: at 49.15% examples, 331281 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:12:41,695 : INFO : EPOCH 5 - PROGRESS: at 51.60% examples, 331130 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:42,725 : INFO : EPOCH 5 - PROGRESS: at 54.10% examples, 331591 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:43,732 : INFO : EPOCH 5 - PROGRESS: at 56.50% examples, 331282 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:12:44,784 : INFO : EPOCH 5 - PROGRESS: at 59.00% examples, 331432 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:45,808 : INFO : EPOCH 5 - PROGRESS: at 61.51% examples, 331684 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:12:46,827 : INFO : EPOCH 5 - PROGRESS: at 63.98% examples, 331737 words/s, in_qsize 11, out_qsize 1\n",
            "2019-04-27 22:12:47,840 : INFO : EPOCH 5 - PROGRESS: at 66.43% examples, 331866 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:48,873 : INFO : EPOCH 5 - PROGRESS: at 68.91% examples, 331735 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:49,874 : INFO : EPOCH 5 - PROGRESS: at 71.32% examples, 331762 words/s, in_qsize 10, out_qsize 1\n",
            "2019-04-27 22:12:50,886 : INFO : EPOCH 5 - PROGRESS: at 73.74% examples, 331654 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:51,904 : INFO : EPOCH 5 - PROGRESS: at 76.22% examples, 331701 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:52,923 : INFO : EPOCH 5 - PROGRESS: at 78.62% examples, 331524 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:53,937 : INFO : EPOCH 5 - PROGRESS: at 81.06% examples, 331430 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:54,947 : INFO : EPOCH 5 - PROGRESS: at 83.62% examples, 331943 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:55,956 : INFO : EPOCH 5 - PROGRESS: at 86.08% examples, 332065 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:56,968 : INFO : EPOCH 5 - PROGRESS: at 88.41% examples, 331790 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:57,999 : INFO : EPOCH 5 - PROGRESS: at 90.88% examples, 331711 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:12:59,008 : INFO : EPOCH 5 - PROGRESS: at 93.36% examples, 331836 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:13:00,051 : INFO : EPOCH 5 - PROGRESS: at 95.87% examples, 331679 words/s, in_qsize 11, out_qsize 0\n",
            "2019-04-27 22:13:01,081 : INFO : EPOCH 5 - PROGRESS: at 98.30% examples, 331629 words/s, in_qsize 12, out_qsize 0\n",
            "2019-04-27 22:13:01,658 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-04-27 22:13:01,708 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-04-27 22:13:01,718 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-04-27 22:13:01,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-04-27 22:13:01,740 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-04-27 22:13:01,745 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-27 22:13:01,749 : INFO : EPOCH - 5 : training on 20621037 raw words (13748802 effective words) took 41.4s, 331998 effective words/s\n",
            "2019-04-27 22:13:01,750 : INFO : training on a 103105185 raw words (68732247 effective words) took 206.7s, 332558 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "EdRxSe5WANg8",
        "colab_type": "code",
        "outputId": "3541e7ea-b7a6-460d-a4db-58a49b13f586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "model.init_sims(replace=True)   # init_sims will make the model much more memory-efficient.\n",
        "\n",
        "model_name = \"drive/300features_40minwords_10context\"\n",
        "model.save(model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-26 19:05:11,947 : INFO : precomputing L2-norms of word weight vectors\n",
            "2019-04-26 19:05:12,052 : INFO : saving Word2Vec object under drive/300features_40minwords_10context, separately None\n",
            "2019-04-26 19:05:12,054 : INFO : not storing attribute vectors_norm\n",
            "2019-04-26 19:05:12,056 : INFO : not storing attribute cum_table\n",
            "2019-04-26 19:05:12,056 : WARNING : this function is deprecated, use smart_open.open instead\n",
            "2019-04-26 19:05:18,240 : INFO : saved drive/300features_40minwords_10context\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mr_wGBJACYOu",
        "colab_type": "code",
        "outputId": "44152dd7-646b-4433-899c-8c25259c5327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "model=Word2Vec.load(\"drive/300features_40minwords_10context\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-26 20:22:47,231 : INFO : loading Word2Vec object from drive/300features_40minwords_10context\n",
            "2019-04-26 20:22:47,232 : WARNING : this function is deprecated, use smart_open.open instead\n",
            "2019-04-26 20:22:49,147 : INFO : loading wv recursively from drive/300features_40minwords_10context.wv.* with mmap=None\n",
            "2019-04-26 20:22:49,148 : INFO : setting ignored attribute vectors_norm to None\n",
            "2019-04-26 20:22:49,149 : INFO : loading vocabulary recursively from drive/300features_40minwords_10context.vocabulary.* with mmap=None\n",
            "2019-04-26 20:22:49,155 : INFO : loading trainables recursively from drive/300features_40minwords_10context.trainables.* with mmap=None\n",
            "2019-04-26 20:22:49,158 : INFO : setting ignored attribute cum_table to None\n",
            "2019-04-26 20:22:49,160 : INFO : loaded drive/300features_40minwords_10context\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HGe1WHTzYCIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def review_to_vector(reviews):\n",
        "  cleaned_vectors=np.zeros((reviews.shape[0],model.wv.syn0.shape[1]))\n",
        "  model_dict_words=model.wv.index2word\n",
        "  for i in range(reviews.shape[0]):\n",
        "    cleaned=clean_text(reviews[i],True)\n",
        "    wc=0\n",
        "    for word in cleaned:\n",
        "      if word in model_dict_words:\n",
        "        wc+=1\n",
        "        cleaned_vectors[i]+=model[word]\n",
        "    cleaned_vectors[i]/=wc\n",
        "  return cleaned_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CbdC6716ppYm",
        "colab_type": "code",
        "outputId": "f793ca75-7d5b-4542-d43a-53f84c40a9cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "cell_type": "code",
      "source": [
        "train_average_vector_data=review_to_vector(train_data.review)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YhjisWe5pv5z",
        "colab_type": "code",
        "outputId": "431e4a02-5a38-4fa7-dd8c-3f1270a2843f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "train_average_vector_data.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "sp_Uw-Qjq7Xb",
        "colab_type": "code",
        "outputId": "af7f850b-02f3-4073-c74d-80dd9e189601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "cell_type": "code",
      "source": [
        "test_average_vector_data=review_to_vector(test_data.review)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9PaVwQfBuIZv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rc=RandomForestClassifier(150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "d9de5619-9062-45b6-eddf-58ff72364069",
        "id": "FigYPFwbuIZy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "cell_type": "code",
      "source": [
        "rc.fit(train_average_vector_data,train_data.sentiment)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
              "            oob_score=False, random_state=None, verbose=0,\n",
              "            warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "aYKoPSJO5IgD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_prediction=rc.predict(test_average_vector_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ps-Obt495s_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output = pd.DataFrame( data={\"id\":test_data[\"id\"], \"sentiment\":test_prediction} )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ymoM4PjkBIIM",
        "colab_type": "code",
        "outputId": "cd12176d-c93b-47e7-e330-2e1539c22c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "d8UjyAxYBL50",
        "colab_type": "code",
        "outputId": "b11a7b96-b407-468d-888b-6220206af974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "output.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"12311_10\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"8348_2\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"5828_4\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"7186_2\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"12128_7\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  sentiment\n",
              "0  \"12311_10\"          1\n",
              "1    \"8348_2\"          0\n",
              "2    \"5828_4\"          1\n",
              "3    \"7186_2\"          0\n",
              "4   \"12128_7\"          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "v8jviL-65v72",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output.to_csv( \"drive/Word2Vec_model.csv\", index=False, quoting=3 )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}